[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Our Vision",
    "section": "",
    "text": "casual.ai\n\n\nCASUAL.AI is a smart outfit recommendation AI agent. The idea behind CASUAL.AI is to make daily outfit decisions faster and smarter. We wanted to solve an everyday problem: “What should I wear today?” - and more importantly, “What goes well together?”\nPeople often spend 15–20 minutes each day choosing an outfit, only to second-guess if it fits the occasion or the weather. CASUAL.AI reduces this decision fatigue by acting like a digital personal stylist.\nIt analyzes your wardrobe, the event type, and real-time weather to offer curated outfit suggestions. Over time, it learns from your choices and preferences to make even smarter recommendations.\nThe goal is to help users make better outfit choices by providing personalized recommendations based on their wardrobe, occasions, and the current weather conditions.\nCASUAL.AI works like a personalized stylist that helps users match clothes based on occasion and weather; just like a digital stylist that understands your wardrobe."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Our Vision",
    "section": "",
    "text": "casual.ai\n\n\nCASUAL.AI is a smart outfit recommendation AI agent. The idea behind CASUAL.AI is to make daily outfit decisions faster and smarter. We wanted to solve an everyday problem: “What should I wear today?” - and more importantly, “What goes well together?”\nPeople often spend 15–20 minutes each day choosing an outfit, only to second-guess if it fits the occasion or the weather. CASUAL.AI reduces this decision fatigue by acting like a digital personal stylist.\nIt analyzes your wardrobe, the event type, and real-time weather to offer curated outfit suggestions. Over time, it learns from your choices and preferences to make even smarter recommendations.\nThe goal is to help users make better outfit choices by providing personalized recommendations based on their wardrobe, occasions, and the current weather conditions.\nCASUAL.AI works like a personalized stylist that helps users match clothes based on occasion and weather; just like a digital stylist that understands your wardrobe."
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "Conclusion"
  },
  {
    "objectID": "data_prep.html",
    "href": "data_prep.html",
    "title": "Data Preprocessing",
    "section": "",
    "text": "We changed the filenames for the sake of data integrity and consistency.\n\n\n\n \nWe trained our agent using Kaggle’s clothing dataset, categorized into pants, shorts, shirts, and t-shirts.\n\n\n\n\n\n\nmetadata\n\n\nWe also created metadata manually including color, gender, and sleeve length"
  },
  {
    "objectID": "data_prep.html#data-preprocessing",
    "href": "data_prep.html#data-preprocessing",
    "title": "Data Preprocessing",
    "section": "",
    "text": "We changed the filenames for the sake of data integrity and consistency.\n\n\n\n \nWe trained our agent using Kaggle’s clothing dataset, categorized into pants, shorts, shirts, and t-shirts.\n\n\n\n\n\n\nmetadata\n\n\nWe also created metadata manually including color, gender, and sleeve length"
  },
  {
    "objectID": "data_prep.html#segmentation",
    "href": "data_prep.html#segmentation",
    "title": "Data Preprocessing",
    "section": "Segmentation",
    "text": "Segmentation\nWe applied image segmentation using HuggingFace’s SAM model to remove the background of the images in order to focus only on the clothing items."
  },
  {
    "objectID": "model_2.html",
    "href": "model_2.html",
    "title": "Model 2",
    "section": "",
    "text": "Siamese Network for Outfit Compatibility\nTo assess how well different clothing items pair together, we implemented a Siamese Network that learns to model outfit compatibility using pairwise embeddings.\nThis model takes input from our previously trained ResNet-18 classifier (Model 1), which generates 512-dimensional feature embeddings for each clothing item.\n\nInputs:\n\nfeature_model.pth: Pre-trained feature extractor (ResNet-based)\ntraining_pairs.json: JSON file containing labeled pairs of compatible and incompatible outfits\n\nThese inputs are used to train the Siamese Network to compare two items and learn a 128-dimensional compatibility embedding that captures how visually and stylistically well two items go together.\n\n\nFeature Extraction and Training:\n\nFor each clothing item pair, 512-D features are extracted using Model 1\nThese features are passed through a Siamese architecture to learn pairwise relationships\nThe output embeddings are trained using contrastive loss, encouraging similar pairs to be close in embedding space and dissimilar pairs to be far apart\n\n\n\nCompatibility Scoring:\nOnce trained, the Siamese model evaluates any pair of items by computing the Euclidean distance between their compatibility embeddings: - Smaller distances indicate higher compatibility - This allows the system to rank or filter clothing items during outfit generation\n\n\nOutput:\n\nThe final trained model is saved as siamese_model.pth\nThis model is used downstream to support color and style pairing logic within the Outfit Pair Builder module\n\n\n\nEvaluation\nTo evaluate the effectiveness of our Siamese Network in modeling outfit compatibility, we conducted an embedding-based similarity search using the final trained model.\nWe began by loading precomputed 512-dimensional feature embeddings generated from Model 1, and then passed them through the trained Siamese model to obtain 128-dimensional compatibility embeddings. These embeddings were stored and indexed using FAISS for efficient similarity lookup.\nWe then used a query-based evaluation strategy, where for a given clothing item , we retrieved the top-k most compatible items from other categories . Compatibility was determined by computing the Euclidean distance between embeddings — the smaller the distance, the more compatible the items were assumed to be.\nTo validate this, we used real wardrobe metadata and checked if the top matches shared consistent style tags . This ensured that retrieval wasn’t just based on visual similarity but also reflected meaningful stylistic compatibility.\n\n\nVisualisation\n\n1. Distance Histogram of Siamese Embeddings\n This histogram visualizes the distribution of Euclidean distances between clothing item pairs in the learned Siamese embedding space. The pairs are split into:\nAs shown in the image, the red distribution (negative pairs) peaks on the left, around 0.2–0.4 The green distribution (positive pairs) peaks around 0.7–0.8 Despite some overlap in the 0.5–0.6 range, the histogram shows a clear separation between compatible and incompatible items, demonstrating that the model is successfully learning to distinguish stylistically matched items from mismatched ones.\nThis evaluation confirms the contrastive loss objective was effective in shaping the embedding space for meaningful outfit compatibility scoring.\n\n\n2. t-SNE Visualization of Embedding Space\n\nThe clear clustering of items by type indicates that the Siamese model has learned a meaningful embedding structure — grouping visually or stylistically similar items close together, while separating distinct types.\nFor instance:\nT-shirts (orange) form a well-defined cluster in the bottom-left, clearly separated from other categories. Shirts (red) are mostly grouped near each other in the top-middle. Bottomwear items like pants (blue) and shorts (green) overlap slightly, but still form recognizable regions. This shows that the model is successfully learning category-aware compatibility features, which improves its ability to suggest valid top-bottom combinations during outfit pairing."
  },
  {
    "objectID": "model_1.html",
    "href": "model_1.html",
    "title": "Model 1",
    "section": "",
    "text": "Once the images were segmented, we extracted features from each using a ResNet-18 model. Resnet is one of the well-known models for image classification, and it was perfect for our data since Resnet is the best model for feature extraction. We were able to achieve 99.3% of accuracy from this model.\nOnce we achieved high accuracy, not only did we use cloth embeddings but we also built a similarity index using Faiss and saved our feature extraction model for use in the later stages.\nWhen evaluating the model, we tested similarity queries for random and top 5 matches, and also analyzed intra- and inter-class distances to confirm whether the embeddings were valid or not. We also visualized the embedding space using t-SNE, which showed clear separation between the categories.\nThese embeddings were essential for the second model, which we’ll get into next.”"
  },
  {
    "objectID": "rag.html",
    "href": "rag.html",
    "title": "RAG",
    "section": "",
    "text": "Retrieval Phase\nThe goal of this phase is to fetch the most contextually appropriate clothing items from the user’s wardrobe. This is achieved using the following components:\n\nFAISS Index for Similarity Search:\nWe use FAISS to store and quickly retrieve clothing item embeddings. Given a user’s query, we use metadata (occasion, temperature, preferences) to search and rank compatible wardrobe items efficiently.\nWeather Tool (OpenWeatherMap API):\nTo ensure climate-appropriate suggestions, we fetch real-time temperature data based on the user’s city. If no city is specified, we default to Washington, DC. This helps us avoid suggesting winter coats on hot summer days or sleeveless tops in winter.\nBelow’s the snippet of code that we’ve built to extract any other city from user’s prompt. It might not be the most effecient if the user doesn’t capitalise the place, however we found it to be a smarter way to use regex to retrieve the desired location.\n\nOccasion Parser:\nThis tool analyzes the user’s natural language input to determine the dressing context — such as a date, job interview, gym session, or casual hangout. It maps keywords in the prompt to predefined occasion categories.\nBelow is the function that we’ve defined to parse the user’s message to fetch the 7 categories of occasions.\n\nPreference Parser:\nUsers often express preferences like “I want to wear a t-shirt, not a shirt.” The preference parser extracts such constraints to guide wardrobe filtering more accurately.\nColor Combination Dictionary:\nWe maintain a curated color compatibility dictionary where each base color is mapped to visually harmonious options. This ensures that the suggested outfits not only make sense contextually, but also look good together.\nBelow is the key value pair of colors that we think go well together, hardcoded in the dictionary format.\n\nWardrobe Filter:\nFinally, all the retrieved metadata — including temperature, occasion, preferences, and color constraints — are used to intelligently narrow down the wardrobe to a set of suitable items. These filtered options form the candidate pool for outfit generation.\nBelow image depicts how the wardrobe filter function ties everything together.\n\n\n\n\nGeneration Phase\nOnce we have the set of filtered and compatible clothing items, we move on to generating the final recommendation using a large language model.\n\nLLM (Anthropic Claude):\nWe query the Claude LLM with a structured prompt that includes:\n\nThe user’s situation\nThe temperature\nA list of 2–3 outfit combinations (top and bottom)\n\nClaude then:\n\nEvaluates the vibe of each outfit (e.g., chill, sporty, bold)\nChooses the most appropriate outfit\nReturns a casual, human-like explanation, similar to texting a fashion-savvy friend\n\n\n\n\nWhy our RAG Setup is Unique\nTraditional RAG systems retrieve text documents and pass them to an LLM.\nOur system, however, retrieves structured visual and metadata representations of clothing, enriched with real-world context like weather and occasion. This makes the recommendations not just accurate, but personalized, practical, and stylish."
  },
  {
    "objectID": "rag.html#rag",
    "href": "rag.html#rag",
    "title": "RAG",
    "section": "",
    "text": "The goal of this phase is to fetch the most contextually appropriate clothing items from the user’s wardrobe. This is achieved using the following components:\n\nFAISS Index for Similarity Search:\nWe use FAISS to store and quickly retrieve clothing item embeddings. Given a user’s query, we use metadata (occasion, temperature, preferences) to search and rank compatible wardrobe items efficiently.\nWeather Tool (OpenWeatherMap API):\nTo ensure climate-appropriate suggestions, we fetch real-time temperature data based on the user’s city. If no city is specified, we default to Washington, DC. This helps us avoid suggesting winter coats on hot summer days or sleeveless tops in winter.\nBelow’s the snippet of code that we’ve built to extract any other city from user’s prompt. It might not be the most effecient if the user doesn’t capitalise the place, however we found it to be a smarter way to use regex to retrieve the desired location.\n\nOccasion Parser:\nThis tool analyzes the user’s natural language input to determine the dressing context — such as a date, job interview, gym session, or casual hangout. It maps keywords in the prompt to predefined occasion categories.\nBelow is the function that we’ve defined to parse the user’s message to fetch the 7 categories of occasions.\n\nPreference Parser:\nUsers often express preferences like “I want to wear a t-shirt, not a shirt.” The preference parser extracts such constraints to guide wardrobe filtering more accurately.\nColor Combination Dictionary:\nWe maintain a curated color compatibility dictionary where each base color is mapped to visually harmonious options. This ensures that the suggested outfits not only make sense contextually, but also look good together.\nBelow is the key value pair of colors that we think go well together, hardcoded in the dictionary format.\n\nWardrobe Filter:\nFinally, all the retrieved metadata — including temperature, occasion, preferences, and color constraints — are used to intelligently narrow down the wardrobe to a set of suitable items. These filtered options form the candidate pool for outfit generation.\nBelow image depicts how the wardrobe filter function ties everything together.\n\n\n\n\n\nOnce we have the set of filtered and compatible clothing items, we move on to generating the final recommendation using a large language model.\n\nLLM (Anthropic Claude):\nWe query the Claude LLM with a structured prompt that includes:\n\nThe user’s situation\nThe temperature\nA list of 2–3 outfit combinations (top and bottom)\n\nClaude then:\n\nEvaluates the vibe of each outfit (e.g., chill, sporty, bold)\nChooses the most appropriate outfit\nReturns a casual, human-like explanation, similar to texting a fashion-savvy friend\n\n\n\n\n\nTraditional RAG systems retrieve text documents and pass them to an LLM.\nOur system, however, retrieves structured visual and metadata representations of clothing, enriched with real-world context like weather and occasion. This makes the recommendations not just accurate, but personalized, practical, and stylish."
  },
  {
    "objectID": "future_work.html",
    "href": "future_work.html",
    "title": "Future Work and Scope of Improvement",
    "section": "",
    "text": "As we continue to refine our intelligent outfit recommendation system, we’ve identified two primary focus areas for future improvements: Metadata Automation and Category Expansion. These enhancements are aimed at making the system more autonomous, stylishly aware, and capable of completing a user’s look from head to toe.\n\nMetadata Automation\nCurrently, metadata such as color, occasion, and sleeve type is manually labeled. To scale and generalize the system for larger wardrobes or more users, we aim to automate this process using vision-based models.\n\nAutomated Color Detection\nWe plan to eliminate the need for manual color tagging by building a color detection module that extracts the dominant color directly from the clothing image. This will improve consistency across wardrobe items and enable better filtering through the color combination dictionary.\n\n\nImage-Based Occasion Detection\nInstead of relying solely on text inputs for determining occasion, we aim to extend occasion detection through image classification. This would allow the system to infer the event type (e.g., formal, sporty, casual) just by analyzing the visual style of a clothing item — making it smarter and reducing dependency on user input.\n\n\nPattern & Texture Recognition\nTo support richer outfit diversity, we plan to introduce pattern recognition capabilities that can identify solids, stripes, florals, polka dots, and more. This feature would not only enhance visual filtering but also help in creating stylistically coherent outfit suggestions.\n\n\nSleeve-Length Integration\nA significant addition will be automated sleeve classification (short, long, sleeveless). The system will then factor in both weather and occasion: - Recommend short sleeves for casual warm-weather occasions - Recommend long sleeves for cold days - Prioritize long sleeves even in warmer weather for formal settings (to maintain dress code standards)\n\n\n\nCategory Expansion\nTo support complete outfit building beyond tops and bottoms, we plan to broaden our system’s understanding and recommendation capacity across additional fashion categories.\n\nAdd New Clothing Categories\nWe plan to include wardrobe categories such as: - Shoes (e.g., sneakers, heels, boots) - Outerwear (e.g., jackets, coats) - Dresses and skirts for more diverse outfit types\nThese additions will help deliver a full look — not just part of it.\n\n\nAccessory Matching Algorithms\nTo take personalization further, we plan to build an accessory pairing engine that recommends: - Bags and backpacks - Jewelry - Belts and watches\nThese final touches will allow our system to move beyond basic styling and deliver fully curated fashion experiences.\n\n\n\nRAG Evaluation Considerations\nAlthough our project is built around a RAG architecture, standard RAG evaluation frameworks like RAGAS may not fully apply in our context.\nHere’s why:\n\nWe’re retrieving structured metadata (wardrobe items with attributes), not text documents.\nOur generation task is creative — generating style vibes and casual outfit suggestions — not fact-based answers.\nFashion is inherently subjective; there is no strict “ground truth” for what’s stylish or appropriate.\n\nThat said, some lightweight evaluation strategies inspired by RAGAS could still be relevant: - Checking if the retrieved outfit matches the user’s weather and occasion context - Evaluating whether the LLM’s final recommendation aligns with the user prompt - Ensuring high parse success rates and clear vibe distinctions\n\n\nFinal Vision\nOur long-term goal is to make the system: - Fully automated in metadata generation\n- Context-aware in outfit recommendations\n- Fashion-forward in understanding trends, patterns, and accessories\n- And most importantly — a true virtual stylist that simplifies decision-making and enhances user confidence\nWe’re building not just an outfit recommender, but a smart styling assistant that adapts, personalizes, and evolves with the user."
  }
]