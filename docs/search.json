[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Our Vision",
    "section": "",
    "text": "casual.ai\n\n\nCASUAL.AI is an intelligent, context-aware outfit recommendation system — your virtual stylist on demand. It was designed to tackle a surprisingly common daily problem:\n“What should I wear today?” and more importantly, “What actually goes well together for what I’m doing?”\n\n\nFor most people, picking the right outfit can be surprisingly stressful. It often takes 15–20 minutes, includes trying on multiple combinations, and ends in last-minute decisions that may not be ideal for the weather, occasion, or setting.\nWe built CASUAL.AI to eliminate this daily friction and bring a sense of confidence and convenience to dressing up. Our system helps users make fast, stylish, and weather-appropriate clothing decisions using the items they already own — no shopping required.\n\n\n\nImagine you have an important job interview at 10 AM, and it’s currently 32°C (90°F) outside.\nYou open CASUAL.AI, type in:\n&gt; “I have an interview today, what should I wear?”\nThe system instantly: - Checks the weather in your city\n- Detects that it’s a formal occasion\n- Filters your wardrobe to remove shorts or casual tees\n- Applies color matching rules\n- Presents you with 3 curated outfit options\n- And finally, asks Anthropic’s Claude LLM to evaluate and suggest the best outfit, along with a natural explanation like:\n&gt; “Go with the navy pants and white long-sleeve — sharp and breathable. Perfect for the weather and gives off a confident, professional vibe.”\n\n\n\n\nUnderstands your wardrobe\n\nKnows the context\n\nGenerates smart suggestions\n\nSaves you time\n\n\n\n\nThe ultimate goal of CASUAL.AI is to simplify and elevate the way users dress.\nWhether you’re heading to: - A formal event\n- A casual hangout\n- A gym session\n- Or even a last-minute dinner\nCASUAL.AI helps you look good, feel prepared, and save time — without overthinking it.\nIt’s more than a recommendation system — it’s a smart, fashion-aware companion that grows with your style."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Our Vision",
    "section": "",
    "text": "casual.ai\n\n\nCASUAL.AI is an intelligent, context-aware outfit recommendation system — your virtual stylist on demand. It was designed to tackle a surprisingly common daily problem:\n“What should I wear today?” and more importantly, “What actually goes well together for what I’m doing?”\n\n\nFor most people, picking the right outfit can be surprisingly stressful. It often takes 15–20 minutes, includes trying on multiple combinations, and ends in last-minute decisions that may not be ideal for the weather, occasion, or setting.\nWe built CASUAL.AI to eliminate this daily friction and bring a sense of confidence and convenience to dressing up. Our system helps users make fast, stylish, and weather-appropriate clothing decisions using the items they already own — no shopping required.\n\n\n\nImagine you have an important job interview at 10 AM, and it’s currently 32°C (90°F) outside.\nYou open CASUAL.AI, type in:\n&gt; “I have an interview today, what should I wear?”\nThe system instantly: - Checks the weather in your city\n- Detects that it’s a formal occasion\n- Filters your wardrobe to remove shorts or casual tees\n- Applies color matching rules\n- Presents you with 3 curated outfit options\n- And finally, asks Anthropic’s Claude LLM to evaluate and suggest the best outfit, along with a natural explanation like:\n&gt; “Go with the navy pants and white long-sleeve — sharp and breathable. Perfect for the weather and gives off a confident, professional vibe.”\n\n\n\n\nUnderstands your wardrobe\n\nKnows the context\n\nGenerates smart suggestions\n\nSaves you time\n\n\n\n\nThe ultimate goal of CASUAL.AI is to simplify and elevate the way users dress.\nWhether you’re heading to: - A formal event\n- A casual hangout\n- A gym session\n- Or even a last-minute dinner\nCASUAL.AI helps you look good, feel prepared, and save time — without overthinking it.\nIt’s more than a recommendation system — it’s a smart, fashion-aware companion that grows with your style."
  },
  {
    "objectID": "index.html#data-preprocessing",
    "href": "index.html#data-preprocessing",
    "title": "Our Vision",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\nFile Renaming for Consistency\nTo ensure data integrity and compatibility across our pipeline, we renamed all clothing image files following a standardized format. This helped maintain consistent references during feature extraction, metadata annotation, and model training.\n\n\n\nDataset Source: Kaggle Fashion Dataset\nWe used a public Kaggle clothing dataset as the primary source for training. The dataset contained images of various clothing items, which we categorized into four main classes:\n\nPants\n\nShorts\n\nShirts\n\nT-shirts\n\nBelow are sample inputs from the dataset:\n\n \n\nThe images varied in resolution, lighting, and orientation — making this dataset an excellent benchmark for building a real-world ready recommendation system.\n\n\n\nManual Metadata Annotation\nTo enhance context-aware outfit generation, we created a custom metadata file (metadata.json) that includes attributes such as:\n\nColor (e.g., black, beige, denim)\n\nCategory (pants, shirt, etc.)\n\nGender (male, female, unisex)\n\nSleeve length (short, long)\n\nThis metadata allowed our system to filter wardrobe items based on occasion, weather, and user preferences."
  },
  {
    "objectID": "index.html#image-segmentation",
    "href": "index.html#image-segmentation",
    "title": "Our Vision",
    "section": "Image Segmentation",
    "text": "Image Segmentation\nTo remove background noise and focus purely on the clothing item, we applied image segmentation using Hugging Face’s Segment Anything Model (SAM).\nSegmenting out the background helped us: - Improve classification accuracy by removing irrelevant visual cues\n- Generate cleaner feature embeddings\n- Build a visually consistent wardrobe UI for users\nThis step was crucial in making sure that the model learned to identify clothes by their actual cut, shape, and style — not the background or setting in which they were photographed."
  },
  {
    "objectID": "rag.html",
    "href": "rag.html",
    "title": "RAG",
    "section": "",
    "text": "Retrieval Phase\nThe goal of this phase is to fetch the most contextually appropriate clothing items from the user’s wardrobe. This is achieved using the following components:\n\nFAISS Index for Similarity Search:\nWe use FAISS to store and quickly retrieve clothing item embeddings. Given a user’s query, we use metadata (occasion, temperature, preferences) to search and rank compatible wardrobe items efficiently.\nWeather Tool (OpenWeatherMap API):\nTo ensure climate-appropriate suggestions, we fetch real-time temperature data based on the user’s city. If no city is specified, we default to Washington, DC. This helps us avoid suggesting winter coats on hot summer days or sleeveless tops in winter.\nBelow’s the snippet of code that we’ve built to extract any other city from user’s prompt. It might not be the most effecient if the user doesn’t capitalise the place, however we found it to be a smarter way to use regex to retrieve the desired location.\n\nOccasion Parser:\nThis tool analyzes the user’s natural language input to determine the dressing context — such as a date, job interview, gym session, or casual hangout. It maps keywords in the prompt to predefined occasion categories.\nBelow is the function that we’ve defined to parse the user’s message to fetch the 7 categories of occasions.\n\nPreference Parser:\nUsers often express preferences like “I want to wear a t-shirt, not a shirt.” The preference parser extracts such constraints to guide wardrobe filtering more accurately.\nColor Combination Dictionary:\nWe maintain a curated color compatibility dictionary where each base color is mapped to visually harmonious options. This ensures that the suggested outfits not only make sense contextually, but also look good together.\nBelow is the key value pair of colors that we think go well together, hardcoded in the dictionary format.\n\nWardrobe Filter:\nFinally, all the retrieved metadata — including temperature, occasion, preferences, and color constraints — are used to intelligently narrow down the wardrobe to a set of suitable items. These filtered options form the candidate pool for outfit generation.\nBelow image depicts how the wardrobe filter function ties everything together.\n\n\n\n\nGeneration Phase\nOnce we have the set of filtered and compatible clothing items, we move on to generating the final recommendation using a large language model.\n\nLLM (Anthropic Claude):\nWe query the Claude LLM with a structured prompt that includes:\n\nThe user’s situation\nThe temperature\nA list of 2–3 outfit combinations (top and bottom)\n\nClaude then:\n\nEvaluates the vibe of each outfit (e.g., chill, sporty, bold)\nChooses the most appropriate outfit\nReturns a casual, human-like explanation, similar to texting a fashion-savvy friend\n\n\n\n\nWhy our RAG Setup is Unique\nTraditional RAG systems retrieve text documents and pass them to an LLM.\nOur system, however, retrieves structured visual and metadata representations of clothing, enriched with real-world context like weather and occasion. This makes the recommendations not just accurate, but personalized, practical, and stylish."
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "Demo and Conclusion",
    "section": "",
    "text": "Your browser does not support the video tag."
  },
  {
    "objectID": "conclusion.html#demo",
    "href": "conclusion.html#demo",
    "title": "Demo and Conclusion",
    "section": "",
    "text": "Your browser does not support the video tag."
  },
  {
    "objectID": "conclusion.html#conclusion",
    "href": "conclusion.html#conclusion",
    "title": "Demo and Conclusion",
    "section": "Conclusion",
    "text": "Conclusion\nThis project set out with a clear and practical mission: to make the user’s life easier and more convenient when it comes to getting dressed. Whether you’re rushing out the door for a meeting or casually planning a weekend brunch, we wanted to eliminate the decision fatigue that often comes with choosing what to wear.\nImagine this: you’ve got 10 minutes before you need to leave, and no time to try on different outfits. With our intelligent fashion assistant, you can simply describe your situation — and get personalized outfit suggestions, complete with compatibility checks, weather readiness, and style explanations, all in seconds.\nTo make this possible, we combined: - A ResNet-18 model for visual understanding of clothing items - A Siamese Network to assess outfit compatibility through learned embeddings - A RAG pipeline with an LLM (Claude) to interpret user intent and recommend outfits with natural, stylistic language - Real-time tools like OpenWeatherMap and FAISS indexing to keep recommendations both relevant and responsive\nThe system retrieves only what’s appropriate, filters for style, occasion, and temperature, and generates a recommendation that feels thoughtful — almost like getting dressed with a fashion-savvy friend.\nLooking ahead, we envision expanding this assistant to include more clothing categories, accessory suggestions, and automated metadata extraction — all with one goal in mind:\nto simplify and elevate the way users dress, every single day."
  },
  {
    "objectID": "future_work.html",
    "href": "future_work.html",
    "title": "Future Work and Scope of Improvement",
    "section": "",
    "text": "As we continue to refine our intelligent outfit recommendation system, we’ve identified two primary focus areas for future improvements: Metadata Automation and Category Expansion. These enhancements are aimed at making the system more autonomous, stylishly aware, and capable of completing a user’s look from head to toe.\n\nMetadata Automation\nCurrently, metadata such as color, occasion, and sleeve type is manually labeled. To scale and generalize the system for larger wardrobes or more users, we aim to automate this process using vision-based models.\n\nAutomated Color Detection\nWe plan to eliminate the need for manual color tagging by building a color detection module that extracts the dominant color directly from the clothing image. This will improve consistency across wardrobe items and enable better filtering through the color combination dictionary.\n\n\nImage-Based Occasion Detection\nInstead of relying solely on text inputs for determining occasion, we aim to extend occasion detection through image classification. This would allow the system to infer the event type (e.g., formal, sporty, casual) just by analyzing the visual style of a clothing item — making it smarter and reducing dependency on user input.\n\n\nPattern & Texture Recognition\nTo support richer outfit diversity, we plan to introduce pattern recognition capabilities that can identify solids, stripes, florals, polka dots, and more. This feature would not only enhance visual filtering but also help in creating stylistically coherent outfit suggestions.\n\n\nSleeve-Length Integration\nA significant addition will be automated sleeve classification (short, long, sleeveless). The system will then factor in both weather and occasion: - Recommend short sleeves for casual warm-weather occasions - Recommend long sleevesfor cold days - Prioritize long sleeves even in warmer weather for formal settings (to maintain dress code standards)\n\n\n\nCategory Expansion\nTo support complete outfit building beyond tops and bottoms, we plan to broaden our system’s understanding and recommendation capacity across additional fashion categories.\n\nAdd New Clothing Categories\nWe plan to include wardrobe categories such as: - Shoes (e.g., sneakers, heels, boots) - Outerwear (e.g., jackets, coats) - Dresses and skirts for more diverse outfit types\nThese additions will help deliver a full look — not just part of it.\n\n\nAccessory Matching Algorithms\nTo take personalization further, we plan to build an accessory pairing engine that recommends: - Bags and backpacks - Jewelry - Belts and watches\nThese final touches will allow our system to move beyond basic styling and deliver fully curated fashion experiences.\n\n\n\nRAG Evaluation Considerations\nAlthough our project is built around a RAG architecture, standard RAG evaluation frameworks like RAGAS may not fully apply in our context.\nHere’s why:\n\nWe’re retrieving structured metadata (wardrobe items with attributes), not text documents.\nOur generation task is creative — generating style vibes and casual outfit suggestions — not fact-based answers.\nFashion is inherently subjective; there is no strict “ground truth” for what’s stylish or appropriate.\n\nThat said, some lightweight evaluation strategies inspired by RAGAS could still be relevant: - Checking if the retrieved outfit matches the user’s weather and occasion context - Evaluating whether the LLM’s final recommendation aligns with the user prompt - Ensuring high parse success rates and clear vibe distinctions\n\n\nFinal Vision\nOur long-term goal is to make the system: - Fully automated in metadata generation\n- Context-aware in outfit recommendations\n- Fashion-forward in understanding trends, patterns, and accessories\n- And most importantly — a true virtual stylist that simplifies decision-making and enhances user confidence\nWe’re building not just an outfit recommender, but a smart styling assistant that adapts, personalizes, and evolves with the user."
  },
  {
    "objectID": "model_2.html",
    "href": "model_2.html",
    "title": "Model 2",
    "section": "",
    "text": "Siamese Network for Outfit Compatibility\nTo assess how well different clothing items pair together, we implemented a Siamese Network that learns to model outfit compatibility using pairwise embeddings.\nThis model takes input from our previously trained ResNet-18 classifier (Model 1), which generates 512-dimensional feature embeddings for each clothing item.\n\nInputs:\n\nfeature_model.pth: Pre-trained feature extractor (ResNet-based)\ntraining_pairs.json: JSON file containing labeled pairs of compatible and incompatible outfits\n\nThese inputs are used to train the Siamese Network to compare two items and learn a 128-dimensional compatibility embedding that captures how visually and stylistically well two items go together.\n\n\nFeature Extraction and Training:\n\nFor each clothing item pair, 512-D features are extracted using Model 1\nThese features are passed through a Siamese architecture to learn pairwise relationships\nThe output embeddings are trained using contrastive loss, encouraging similar pairs to be close in embedding space and dissimilar pairs to be far apart\n\n\n\nCompatibility Scoring:\nOnce trained, the Siamese model evaluates any pair of items by computing the Euclidean distance between their compatibility embeddings: - Smaller distances indicate higher compatibility - This allows the system to rank or filter clothing items during outfit generation\n\n\nOutput:\n\nThe final trained model is saved as siamese_model.pth\nThis model is used downstream to support color and style pairing logic within the Outfit Pair Builder module\n\n\n\nEvaluation\nTo evaluate the effectiveness of our Siamese Network in modeling outfit compatibility, we conducted an embedding-based similarity search using the final trained model.\nWe began by loading precomputed 512-dimensional feature embeddings generated from Model 1, and then passed them through the trained Siamese model to obtain 128-dimensional compatibility embeddings. These embeddings were stored and indexed using FAISS for efficient similarity lookup.\nWe then used a query-based evaluation strategy, where for a given clothing item , we retrieved the top-k most compatible items from other categories . Compatibility was determined by computing the Euclidean distance between embeddings — the smaller the distance, the more compatible the items were assumed to be.\nTo validate this, we used real wardrobe metadata and checked if the top matches shared consistent style tags . This ensured that retrieval wasn’t just based on visual similarity but also reflected meaningful stylistic compatibility.\n\n\nVisualisation\n\n1. Distance Histogram of Siamese Embeddings\n This histogram visualizes the distribution of Euclidean distances between clothing item pairs in the learned Siamese embedding space. The pairs are split into:\nAs shown in the image, the red distribution (negative pairs) peaks on the left, around 0.2–0.4 The green distribution (positive pairs) peaks around 0.7–0.8 Despite some overlap in the 0.5–0.6 range, the histogram shows a clear separation between compatible and incompatible items, demonstrating that the model is successfully learning to distinguish stylistically matched items from mismatched ones.\nThis evaluation confirms the contrastive loss objective was effective in shaping the embedding space for meaningful outfit compatibility scoring.\n\n\n2. t-SNE Visualization of Embedding Space\n\nThe clear clustering of items by type indicates that the Siamese model has learned a meaningful embedding structure — grouping visually or stylistically similar items close together, while separating distinct types.\nFor instance:\nT-shirts (orange) form a well-defined cluster in the bottom-left, clearly separated from other categories. Shirts (red) are mostly grouped near each other in the top-middle. Bottomwear items like pants (blue) and shorts (green) overlap slightly, but still form recognizable regions. This shows that the model is successfully learning category-aware compatibility features, which improves its ability to suggest valid top-bottom combinations during outfit pairing.\n\n\nTop query match\nThis is how the top outfit matches look like when provided the query"
  },
  {
    "objectID": "model_1.html",
    "href": "model_1.html",
    "title": "Model 1",
    "section": "",
    "text": "Before any modeling, we begin by segmenting the uploaded clothing images to remove background distractions and isolate just the clothing item. This step ensures clean inputs for our classifier.\nOnce segmented, we pass each image through a ResNet-18 classifier, trained to categorize items into one of four clothing types: shirts, t-shirts, pants, and shorts.\n\n\nWe chose this model because it’s a widely used model for image classification and has proven effective for visual feature extraction due to its residual connections and lightweight depth.\nIt was ideal for our task because it allowed us to both: - Classify clothing items accurately - Extract visual embeddings that could be used for downstream similarity and compatibility tasks\nWe achieved a high classification accuracy of 99.3%, validating the quality of the model’s learning.\n\n\n\n\nSegmented clothing images\nLabels for 4 clothing categories: pants, shorts, shirts, t-shirts\n\n\n\n\nAfter training: - We repurposed the ResNet model as a feature extractor, using the 512-dimensional vector from its penultimate layer. - These embeddings capture details like shape, fit, collar type, and texture. - All embeddings were saved in clothing_embedding.json.\nTo support fast similarity search, we built a FAISS index and saved it as clothing_faiss.index. This allowed us to later retrieve visually similar items efficiently — a crucial step for building outfit recommendations.\n\n\n\n\nTrained model weights: feature_model.pth\nEmbedding storage: clothing_embedding.json\nFAISS index for similarity: clothing_faiss.index"
  },
  {
    "objectID": "model_1.html#resnet-18-classifier",
    "href": "model_1.html#resnet-18-classifier",
    "title": "Model 1",
    "section": "",
    "text": "Before any modeling, we begin by segmenting the uploaded clothing images to remove background distractions and isolate just the clothing item. This step ensures clean inputs for our classifier.\nOnce segmented, we pass each image through a ResNet-18 classifier, trained to categorize items into one of four clothing types: shirts, t-shirts, pants, and shorts.\n\n\nWe chose this model because it’s a widely used model for image classification and has proven effective for visual feature extraction due to its residual connections and lightweight depth.\nIt was ideal for our task because it allowed us to both: - Classify clothing items accurately - Extract visual embeddings that could be used for downstream similarity and compatibility tasks\nWe achieved a high classification accuracy of 99.3%, validating the quality of the model’s learning.\n\n\n\n\nSegmented clothing images\nLabels for 4 clothing categories: pants, shorts, shirts, t-shirts\n\n\n\n\nAfter training: - We repurposed the ResNet model as a feature extractor, using the 512-dimensional vector from its penultimate layer. - These embeddings capture details like shape, fit, collar type, and texture. - All embeddings were saved in clothing_embedding.json.\nTo support fast similarity search, we built a FAISS index and saved it as clothing_faiss.index. This allowed us to later retrieve visually similar items efficiently — a crucial step for building outfit recommendations.\n\n\n\n\nTrained model weights: feature_model.pth\nEmbedding storage: clothing_embedding.json\nFAISS index for similarity: clothing_faiss.index"
  },
  {
    "objectID": "model_1.html#evaluation",
    "href": "model_1.html#evaluation",
    "title": "Model 1",
    "section": "Evaluation",
    "text": "Evaluation\nWe evaluated the effectiveness of the embeddings in representing clothing similarity through three strategies:\n\n1. Random and Top-K Visual Matches\nWe ran visual similarity queries using random clothing items and validated whether the top retrieved matches were perceptually similar. Matches were manually verified to have similar structure, texture, and category.\n\n\n2. Intra- and Inter-Class Distance Analysis\nWe computed the average distances within each class (e.g., t-shirts with t-shirts) and across different classes (e.g., t-shirts vs pants). The significant gap between intra- and inter-class distances confirmed that the embeddings were discriminative and structured.\n\n\n\n3. t-SNE Visualization\nTo visually inspect the structure of the learned embedding space, we projected the 512-D vectors to 2D using t-SNE. This confirmed strong clustering by category, indicating that similar items were embedded close together."
  },
  {
    "objectID": "model_1.html#visualisation",
    "href": "model_1.html#visualisation",
    "title": "Model 1",
    "section": "Visualisation",
    "text": "Visualisation\n\nRandom Visual Similarity Search\nWe validated the embedding quality using similarity queries:\n\n\n\nt-SNE Visualization of Clothing Embeddings\nWe reduced the high-dimensional embedding space to 2D using t-SNE and color-coded each category:\n\nClear visual clustering of pants, shirts, t-shirts, and shorts confirmed the model’s ability to learn semantically meaningful embeddings.\n\n\nTransition to Model 2\nThese high-quality embeddings formed the foundation for our second model — the Siamese Network — which uses these vectors to learn pairwise compatibility and outfit matching.\nLet’s move on to how we modeled style and compatibility using those embeddings in Model 2."
  }
]