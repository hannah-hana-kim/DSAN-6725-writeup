{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Model 1\n",
    "format:\n",
    "  html:\n",
    "    embed-resources: true\n",
    "    code-fold: false\n",
    "execute:\n",
    "  echo: true\n",
    "  warning: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ResNet-18 Classifier \n",
    "\n",
    "Before any modeling, we begin by segmenting the uploaded clothing images to remove background distractions and isolate just the clothing item. This step ensures clean inputs for our classifier.\n",
    "\n",
    "Once segmented, we pass each image through a ResNet-18 classifier, trained to categorize items into one of four clothing types: shirts, t-shirts, pants, and shorts.\n",
    "\n",
    "\n",
    "### Why ResNet-18?\n",
    "\n",
    "We chose this model because it's a widely used model for image classification and has proven effective for visual feature extraction due to its residual connections and lightweight depth.  \n",
    "It was ideal for our task because it allowed us to both:\n",
    "- Classify clothing items accurately\n",
    "- Extract visual embeddings that could be used for downstream similarity and compatibility tasks\n",
    "\n",
    "We achieved a high classification accuracy of 99.3%, validating the quality of the model’s learning.\n",
    "\n",
    "\n",
    "### Inputs:\n",
    "- Segmented clothing images\n",
    "- Labels for 4 clothing categories: `pants`, `shorts`, `shirts`, `t-shirts`\n",
    "\n",
    "### Feature Extraction & Similarity Indexing:\n",
    "\n",
    "After training:\n",
    "- We repurposed the ResNet model as a feature extractor, using the 512-dimensional vector from its penultimate layer.\n",
    "- These embeddings capture details like shape, fit, collar type, and texture.\n",
    "- All embeddings were saved in `clothing_embedding.json`.\n",
    "\n",
    "To support fast similarity search, we built a FAISS index and saved it as `clothing_faiss.index`. This allowed us to later retrieve visually similar items efficiently — a crucial step for building outfit recommendations.\n",
    "\n",
    "### Output:\n",
    "- Trained model weights: `feature_model.pth`\n",
    "- Embedding storage: `clothing_embedding.json`\n",
    "- FAISS index for similarity: `clothing_faiss.index`\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "We evaluated the effectiveness of the embeddings in representing clothing similarity through three strategies:\n",
    "\n",
    "### 1. Random and Top-K Visual Matches\n",
    "We ran visual similarity queries using random clothing items and validated whether the top retrieved matches were perceptually similar. Matches were manually verified to have similar structure, texture, and category.\n",
    "\n",
    "### 2. Intra- and Inter-Class Distance Analysis\n",
    "We computed the average distances within each class (e.g., t-shirts with t-shirts) and across different classes (e.g., t-shirts vs pants). The significant gap between intra- and inter-class distances confirmed that the embeddings were discriminative and structured.\n",
    "\n",
    "<img src=\"images/model1_distance.png\" alt=\"Distance\" width=\"200\">\n",
    "\n",
    "### 3. t-SNE Visualization\n",
    "To visually inspect the structure of the learned embedding space, we projected the 512-D vectors to 2D using t-SNE. This confirmed strong clustering by category, indicating that similar items were embedded close together.\n",
    "\n",
    "\n",
    "## Visualisation\n",
    "\n",
    "### Random Visual Similarity Search\n",
    "\n",
    "We validated the embedding quality using similarity queries:\n",
    "\n",
    "<img src=\"images/model1_search.png\" alt=\"Visual Similarity Search Samples\" width=\"650\">\n",
    "\n",
    "\n",
    "### t-SNE Visualization of Clothing Embeddings\n",
    "\n",
    "We reduced the high-dimensional embedding space to 2D using t-SNE and color-coded each category:\n",
    "\n",
    "<img src=\"images/model1_tsne.png\" alt=\"t-SNE Clothing Embedding Visualization\" width=\"600\">\n",
    "\n",
    "Clear visual clustering of pants, shirts, t-shirts, and shorts confirmed the model's ability to learn semantically meaningful embeddings.\n",
    "\n",
    "\n",
    "### Transition to Model 2\n",
    "\n",
    "These high-quality embeddings formed the foundation for our second model — the Siamese Network — which uses these vectors to learn pairwise compatibility and outfit matching.\n",
    "\n",
    "Let’s move on to how we modeled style and compatibility using those embeddings in Model 2.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
